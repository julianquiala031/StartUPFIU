{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtMi8U8MyhM2"
      },
      "source": [
        "# Importing Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5yIOUcsAhn6"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV2Ii8DgAmM-"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import pandas\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "!pip install cleantext\n",
        "!pip install pyLDAvis\n",
        "from cleantext import clean\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis.lda_model\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXntpi7rA9Cw"
      },
      "source": [
        "# Data Pre-processesing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F5nWJM2AyDb"
      },
      "outputs": [],
      "source": [
        "# reads json file containing caption\n",
        "user_input = input(\"Please enter the file you would like to process and model. \")\n",
        "df = pandas.read_json(user_input)\n",
        "\n",
        "custom_stop_words = ['startupfiu', 'fiu', 'startup', 'http', 'skydeck', 'berkeley', 'miami', 'ca', 'us','cal', 'uc']\n",
        "std_stop_words = stopwords.words('english')\n",
        "std_stop_words += list(string.punctuation)\n",
        "stop_words = custom_stop_words + std_stop_words\n",
        "\n",
        "#tokenizes, removes: emoji, numbers, punctuation, and makes all words lowercase\n",
        "def tokenize_lowercase(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    new_text = [word for word in tokens if word.isalpha()]\n",
        "    stopwords_removed = [token.lower() for token in new_text if token.lower() not in stop_words]\n",
        "    return stopwords_removed\n",
        "\n",
        "df['caption'] = df['caption'].apply(tokenize_lowercase)\n",
        "\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    if isinstance(sentence, list):  # If input is a list of tokens\n",
        "        sentence = \" \".join(sentence)  # Convert list to string\n",
        "    # Tokenize the sentence and find the POS tag for each token\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "    # Convert nltk POS tags to WordNet tags\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    # Lemmatize each word based on its POS tag\n",
        "    lemmatized_tokens = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            lemmatized_tokens.append(word)\n",
        "        else:\n",
        "            lemmatized_tokens.append(lemmatizer.lemmatize(word, tag))\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Apply the lemmatization function directly to the 'caption' column of the DataFrame\n",
        "df['lemmatized_caption'] = df['caption'].apply(lemmatize_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reads json file containing caption\n",
        "user_input = input(\"Please enter the file you would like to process and model. \")\n",
        "df = pandas.read_json(user_input)\n",
        "\n",
        "custom_stop_words = ['startupfiu', 'fiu', 'startup', 'http', 'skydeck', 'berkeley', 'miami', 'ca', 'us','cal', 'uc']\n",
        "std_stop_words = stopwords.words('english')\n",
        "std_stop_words += list(string.punctuation)\n",
        "stop_words = custom_stop_words + std_stop_words\n",
        "\n",
        "#tokenizes, removes: emoji, numbers, punctuation, and makes all words lowercase\n",
        "def tokenize_lowercase(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    new_text = [word for word in tokens if word.isalpha()]\n",
        "    stopwords_removed = [token.lower() for token in new_text if token.lower() not in stop_words]\n",
        "    return stopwords_removed\n",
        "\n",
        "df['text'] = df['text'].apply(tokenize_lowercase)\n",
        "\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    if isinstance(sentence, list):  # If input is a list of tokens\n",
        "        sentence = \" \".join(sentence)  # Convert list to string\n",
        "    # Tokenize the sentence and find the POS tag for each token\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "    # Convert nltk POS tags to WordNet tags\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    # Lemmatize each word based on its POS tag\n",
        "    lemmatized_tokens = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            lemmatized_tokens.append(word)\n",
        "        else:\n",
        "            lemmatized_tokens.append(lemmatizer.lemmatize(word, tag))\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Apply the lemmatization function directly to the 'caption' column of the DataFrame\n",
        "df['lemmatized_caption'] = df['text'].apply(lemmatize_sentence)"
      ],
      "metadata": {
        "id": "imVVMwRr6O77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rEZykz1lu2b"
      },
      "source": [
        "# Creating skLearn Topic Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ioKOz51RVn9"
      },
      "outputs": [],
      "source": [
        "# Example text data\n",
        "documents = df['lemmatized_caption'].astype(str)\n",
        "\n",
        "# Step 1: Create a document-term matrix\n",
        "vectorizer = CountVectorizer(stop_words=stop_words, max_df=0.85)\n",
        "doc_term_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Step 2: Train the LDA model\n",
        "num_topics = 5 # Number of topics\n",
        "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "lda_model.fit(doc_term_matrix)\n",
        "\n",
        "# Step 3: Interpret the topics\n",
        "def display_topics(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "\n",
        "num_top_words = 10  # Number of top words to display for each topic\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "display_topics(lda_model, feature_names, num_top_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thGMd2sAtuDv"
      },
      "source": [
        "#Average likes for each topic in SKLearn LDA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyohbzUl3zf9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Transform the documents to topic distribution\n",
        "topic_distribution = lda_model.transform(doc_term_matrix)\n",
        "\n",
        "# Assign the dominant topic to each document\n",
        "df['dominant_topic'] = np.argmax(topic_distribution, axis=1)\n",
        "\n",
        "average_likes_per_topic_LDA = df.groupby('dominant_topic')['likesCount'].mean()\n",
        "\n",
        "print(average_likes_per_topic_LDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0-8mhq48Gv5"
      },
      "source": [
        "#Graph of likes in SKLearn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqEdKkBZtHix"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "average_likes_per_topic_LDA.plot(kind='bar', color='skyblue')\n",
        "plt.xlabel('Topic Number')\n",
        "plt.ylabel('Average Likes')\n",
        "plt.title('Average Likes Per Topic')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZRKAQ_ykE11"
      },
      "source": [
        "#Visualizing SkLearn Topic Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cq93RPdekHm5"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "sklearn_display = pyLDAvis.lda_model.prepare(lda_model, doc_term_matrix, vectorizer, sort_topics=False)\n",
        "pyLDAvis.display(sklearn_display)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhznTgrEBA8_"
      },
      "source": [
        "#Training and creating a genSim Topic Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iSHi8tmBDua"
      },
      "outputs": [],
      "source": [
        "# Read data from JSON file into DataFrame\n",
        "df['tokenized_caption'] = df['lemmatized_caption']\n",
        "\n",
        "# Create a dictionary from the preprocessed captions\n",
        "dictionary = Dictionary(df['tokenized_caption'])\n",
        "\n",
        "# Filter out tokens that appear in less than 5 documents or more than 50% of documents\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "\n",
        "# Convert the captions to bag-of-words representation\n",
        "corpus = [dictionary.doc2bow(caption) for caption in df['tokenized_caption']]\n",
        "\n",
        "# Build the LDA model\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=42, passes=20, iterations=400, eval_every=1)\n",
        "\n",
        "# Print the topics\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f\"Topic {idx}: {topic}\")\n",
        "\n",
        "# Calculate coherence score to evaluate the model\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=df['tokenized_caption'], dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print(f\"\\nCoherence Score: {coherence_lda}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3LG2ziIBO7j"
      },
      "source": [
        "#Extracting captions and likes from gensim generated topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-zHO3_uBUwJ"
      },
      "outputs": [],
      "source": [
        "# Extract topics from the model\n",
        "topics = lda_model.show_topics(formatted=False)\n",
        "\n",
        "# Assign topics to captions\n",
        "df['topic'] = df['tokenized_caption'].apply(lambda x: sorted(lda_model.get_document_topics(dictionary.doc2bow(x)), key=lambda x: -x[1])[0][0])\n",
        "\n",
        "# Calculate average likes for each topic\n",
        "average_likes_per_topic_gensim = df.groupby('topic')['likesCount'].mean()\n",
        "\n",
        "print(average_likes_per_topic_gensim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG9n0sISBe-I"
      },
      "source": [
        "#Visualizing gensim Topic Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5zUwnE7Bi1r"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "lda_display = gensimvis.prepare(lda_model, corpus, dictionary, sort_topics=False)\n",
        "pyLDAvis.display(lda_display)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9alonUuQtLUZ"
      },
      "source": [
        "#Visualizing likes per gensim topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6SChYVRtQPN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "average_likes_per_topic_gensim.plot(kind='bar', color='skyblue')\n",
        "plt.xlabel('Topic Number')\n",
        "plt.ylabel('Average Likes')\n",
        "plt.title('Average Likes Per Topic')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "one_GKgo1RST"
      },
      "source": [
        "# NMF Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md470gsV1UIF"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import NMF\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "num_topics = 5\n",
        "\n",
        "# Creating NMF model\n",
        "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
        "nmf_model.fit(doc_term_matrix)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Display topics\n",
        "def display_nmf_topics(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic #%d:\" % topic_idx)\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "\n",
        "def display_topic_words(n_top_words):\n",
        "    display_nmf_topics(nmf_model, feature_names, n_top_words)\n",
        "\n",
        "# Create a slider for choosing number of top words\n",
        "slider = widgets.IntSlider(value=10, min=1, max=20, step=1, description='Top words:')\n",
        "widgets.interactive(display_topic_words, n_top_words=slider)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qgpH9FY1ZK3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Transform the documents to topic distribution\n",
        "topic_distribution = nmf_model.transform(doc_term_matrix)\n",
        "\n",
        "# Assign the dominant topic to each document\n",
        "df['dominant_topic'] = np.argmax(topic_distribution, axis=1)\n",
        "\n",
        "average_likes_per_topic = df.groupby('dominant_topic')['likesCount'].mean()\n",
        "\n",
        "print(average_likes_per_topic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASZeqhhB3Sd0"
      },
      "source": [
        "#Graph of average likes per topic for NMF Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJrHTUQS1eqi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "average_likes_per_topic.plot(kind='bar', color='skyblue')\n",
        "plt.xlabel('Topic Number')\n",
        "plt.ylabel('Average Likes')\n",
        "plt.title('Average Likes Per Topic')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwrNPyX-1tUv"
      },
      "source": [
        "#Vizualize data in word cloud for NMF Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wcgGnLz1wqj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from collections import defaultdict\n",
        "\n",
        "!pip install wordcloud\n",
        "\n",
        "def prepare_word_cloud_data(df):\n",
        "    # grouping data by dominant_topic,aggregating likesCount with sum, concatenating tokens\n",
        "    aggregated_data = df.groupby('dominant_topic').agg({\n",
        "        'likesCount': 'sum',\n",
        "        'tokenized_caption': lambda x: ' '.join([' '.join(tokens) for tokens in x])\n",
        "    }).to_dict(orient='index')\n",
        "\n",
        "    # word clouds for each topic weighted by likesCount\n",
        "    for topic, data in aggregated_data.items():\n",
        "\n",
        "        wordcloud = WordCloud(width=800, height=400, max_words=200).generate_from_text(data['tokenized_caption'])\n",
        "\n",
        "        # display image\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.title(f'Word Cloud for Topic {topic} (Total Likes: {data[\"likesCount\"]})')\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "prepare_word_cloud_data(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoDlDPVhmlwW"
      },
      "source": [
        "#Visualizing NMF Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c5shT8smpG9"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "\n",
        "# Ensure 'documents' is a list of all your document strings\n",
        "doc_lengths = [len(doc.split()) for doc in documents]\n",
        "\n",
        "# Vocabulary and term frequencies from the CountVectorizer\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "term_frequency = np.asarray(doc_term_matrix.sum(axis=0)).ravel().tolist()\n",
        "\n",
        "# Adding a small number to avoid division by zero\n",
        "epsilon = 1e-6\n",
        "doc_topic_dists = nmf_model.transform(doc_term_matrix) + epsilon\n",
        "doc_topic_dists = doc_topic_dists / doc_topic_dists.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Normalize topic-term distributions\n",
        "topic_term_dists = nmf_model.components_\n",
        "topic_term_dists = topic_term_dists / topic_term_dists.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Check if normalization was successful\n",
        "if np.any(np.isnan(doc_topic_dists)):\n",
        "    raise ValueError(\"NaN values found in document-topic distributions after normalization.\")\n",
        "\n",
        "# Prepare the data for pyLDAvis\n",
        "data = {\n",
        "    'topic_term_dists': topic_term_dists,\n",
        "    'doc_topic_dists': doc_topic_dists,\n",
        "    'doc_lengths': doc_lengths,\n",
        "    'vocab': vocab,\n",
        "    'term_frequency': term_frequency\n",
        "}\n",
        "\n",
        "# Generate the visualization\n",
        "vis_data = pyLDAvis.prepare(**data)\n",
        "pyLDAvis.display(vis_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xS5GlI2yU78"
      },
      "source": [
        "\n",
        "# Synthetic Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bcBfBz5yvEa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln1hsl0OynTb"
      },
      "outputs": [],
      "source": [
        "# Load existing data from Excel sheet\n",
        "existing_data = pd.read_excel(\"existing_data.xlsx\")\n",
        "\n",
        "# Generate synthetic data\n",
        "num_events = len(existing_data)\n",
        "\n",
        "# Generate synthetic event names\n",
        "synthetic_event_names = [\"Synthetic Event \" + str(i+1) for i in range(num_events)]\n",
        "\n",
        "# Generate synthetic attendee numbers using a normal distribution\n",
        "mean_attendees = existing_data[\"# Attendes\"].mean()\n",
        "std_attendees = existing_data[\"# Attendes\"].std()\n",
        "synthetic_attendees = np.round(np.random.normal(mean_attendees, std_attendees, num_events))\n",
        "\n",
        "# Generate synthetic dates within a specified range (e.g., 2024-01-01 to 2024-12-31)\n",
        "start_date = pd.to_datetime('2024-01-01')\n",
        "end_date = pd.to_datetime('2024-12-31')\n",
        "synthetic_dates = pd.date_range(start_date, end_date, periods=num_events)\n",
        "\n",
        "# Generate synthetic activation statuses (1 or 0)\n",
        "synthetic_activation = np.random.choice([0, 1], size=num_events)\n",
        "\n",
        "# Create a DataFrame for synthetic data\n",
        "synthetic_data = pd.DataFrame({\n",
        "    \"Name of the Event\": synthetic_event_names,\n",
        "    \"# Attendes\": synthetic_attendees,\n",
        "    \"Date\": synthetic_dates,\n",
        "    \"Activation\": synthetic_activation\n",
        "})\n",
        "\n",
        "# Save synthetic data to Excel\n",
        "synthetic_data.to_excel(\"synthetic_data.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnSTSULGKfTk"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABQwVpS5KguL"
      },
      "source": [
        "# Other Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLoUz9sSKj3E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to your JSON files\n",
        "file_paths = ['path_to_first_file.json', 'path_to_second_file.json', 'path_to_third_file.json']\n",
        "\n",
        "# Load each file into a DataFrame and append it to a list\n",
        "dataframes = [pd.read_json(path) for path in file_paths]\n",
        "\n",
        "# Concatenate all DataFrames into one\n",
        "combined_dataframe = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Now you can save it or do whatever you want with it\n",
        "# Save the combined DataFrame to a new JSON file, for example\n",
        "combined_dataframe.to_json('combined_file.json', orient='records', lines=True)\n",
        "\n",
        "# If you want to work with it further, you can do that too\n",
        "print(combined_dataframe.head())  # Just to check the first few rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yai-Jh9vv_c-"
      },
      "source": [
        "#LinkedIn Visualization (by Frank so may be ugly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVWFkI6Wv-5t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/linkedin_profiles.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jE_N3anqxF03"
      },
      "outputs": [],
      "source": [
        "# Calculate the percentage of profiles per industry\n",
        "industry_counts = data['Industries'].value_counts(normalize=True) * 100  # This gives you percentages\n",
        "\n",
        "# Convert to DataFrame for plotting\n",
        "df = industry_counts.reset_index()\n",
        "df.columns = ['Industry', 'Percentage']\n",
        "\n",
        "# Group smaller industries into 'Other'\n",
        "threshold = 2  # Percentage threshold to group into 'Other'\n",
        "mask = df['Percentage'] < threshold\n",
        "df.loc[mask, 'Industry'] = 'Other'  # Grouping smaller categories into 'Other'\n",
        "\n",
        "# Ensure 'Other' is combined into one row if there are multiple 'Other' entries\n",
        "df = df.groupby('Industry')['Percentage'].sum().reset_index()\n",
        "df.sort_values('Percentage', ascending=False, inplace=True)\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 8))  # Increased figure size\n",
        "pie = df.set_index('Industry')['Percentage'].plot(kind='pie', autopct='%1.1f%%', startangle=90, counterclock=False, colors=plt.get_cmap('tab20').colors, ax=ax, pctdistance=0.85, fontsize=10)\n",
        "centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
        "fig.gca().add_artist(centre_circle)\n",
        "\n",
        "# Explicitly hide axis labels\n",
        "ax.set_ylabel('')  # This hides the y-axis label\n",
        "\n",
        "# Adjust legend\n",
        "ax.legend(df['Industry'], title='Industry', loc='center left', bbox_to_anchor=(1, 0.5), fontsize=9)\n",
        "ax.axis('equal')  # Ensure pie is drawn as a circle\n",
        "\n",
        "plt.title('Industry Breakdown', color='black', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV_X5yjExKfs"
      },
      "outputs": [],
      "source": [
        "# Skill Word Cloud\n",
        "all_skills = ' '.join(data['Skills'].dropna())\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_skills)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpHzHvgfxN59"
      },
      "outputs": [],
      "source": [
        "# Top Degrees\n",
        "degree_counts = data['Degree Name'].value_counts().head(10)\n",
        "degree_counts.plot(kind='bar', figsize=(10, 5))\n",
        "plt.title('Top Degrees')\n",
        "plt.xlabel('Degree')\n",
        "plt.ylabel('Number of Profiles')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "668dLOnnxR5o"
      },
      "outputs": [],
      "source": [
        "# Top Schools\n",
        "school_counts = data['School Name'].value_counts().head(10)\n",
        "school_counts.plot(kind='bar', color='green', figsize=(10, 5))\n",
        "plt.title('Top Schools')\n",
        "plt.xlabel('School')\n",
        "plt.ylabel('Number of Alumni')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF9t1VhFxywe"
      },
      "outputs": [],
      "source": [
        "# Bar chart with logarithmic scale for better visibility of data with large disparities\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "country_counts = geo_data['Geo Country Name'].value_counts()\n",
        "country_counts.plot(kind='bar', ax=ax, color='skyblue', logy=True)  # Adding logy=True to apply logarithmic scale\n",
        "ax.set_xlabel('Country')\n",
        "ax.set_ylabel('Number of Entries (Log Scale)')\n",
        "ax.set_title('Breakdown of Data per Country with Logarithmic Scale')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF9KMZSp1T5m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "!pip install googlemaps\n",
        "import googlemaps\n",
        "\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# Your Google Maps API key\n",
        "gmaps = googlemaps.Client(key='AIzaSyBWQDKHu7qGqy35uvNlbR8rjHHpsejIXo4')\n",
        "\n",
        "# Load your data\n",
        "file_path = '/content/linkedin_profiles.csv'  # Replace with the path to your CSV file\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Ensure all values in the 'Location' column are strings\n",
        "data['Location'] = data['Location'].astype(str)\n",
        "\n",
        "# Extract city names from the \"Location\" column\n",
        "data['City'] = data['Location'].apply(lambda x: x.split(',')[0].strip())\n",
        "\n",
        "# Remove duplicates to reduce the number of API requests\n",
        "unique_cities = data['City'].drop_duplicates()\n",
        "\n",
        "# Ensure that 'City' is of type str in unique_cities\n",
        "unique_cities = unique_cities.astype(str)\n",
        "\n",
        "# Function to fetch coordinates\n",
        "def get_coordinates(city):\n",
        "    try:\n",
        "        geocode_result = gmaps.geocode(city)\n",
        "        if geocode_result:\n",
        "            location = geocode_result[0]['geometry']['location']\n",
        "            return location['lat'], location['lng']\n",
        "        else:\n",
        "            return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching coordinates for {city}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Get coordinates for each city\n",
        "coordinates = unique_cities.apply(lambda city: get_coordinates(city))\n",
        "coordinates_df = pd.DataFrame(coordinates.tolist(), index=unique_cities.index, columns=['Latitude', 'Longitude'])\n",
        "\n",
        "# Print the coordinates DataFrame for debugging\n",
        "print(\"Coordinates DataFrame:\")\n",
        "print(coordinates_df.head())\n",
        "\n",
        "# Ensure that 'City' is of type str in data DataFrame\n",
        "data['City'] = data['City'].astype(str)\n",
        "\n",
        "# Ensure the index of coordinates_df is of type str\n",
        "coordinates_df.index = coordinates_df.index.astype(str)\n",
        "\n",
        "# Merge the coordinates back to the original dataframe using pd.concat\n",
        "data = pd.concat([data.set_index('City'), coordinates_df], axis=1, join='inner').reset_index()\n",
        "\n",
        "# Print the merged DataFrame for debugging\n",
        "print(\"Merged DataFrame:\")\n",
        "print(data.head())\n",
        "\n",
        "# Create a GeoDataFrame\n",
        "data['Coordinates'] = data.apply(lambda row: Point(row['Longitude'], row['Latitude']) if pd.notnull(row['Longitude']) and pd.notnull(row['Latitude']) else None, axis=1)\n",
        "gdf = gpd.GeoDataFrame(data, geometry='Coordinates')\n",
        "\n",
        "# Filter out rows with missing coordinates\n",
        "gdf = gdf[gdf['Coordinates'].notnull()]\n",
        "\n",
        "# Print the GeoDataFrame for debugging\n",
        "print(\"GeoDataFrame:\")\n",
        "print(gdf.head())\n",
        "\n",
        "# Load a world map\n",
        "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "world.plot(ax=ax, color='lightgrey')\n",
        "gdf.plot(ax=ax, marker='o', color='red', markersize=5)\n",
        "ax.set_title('World Map Showing City Locations')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQBDu8oQ5YY0"
      },
      "source": [
        "#Sentiment Analysis using VADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7WlaoaO__eQ"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(captions):\n",
        "  return analyzer.polarity_scores(captions)\n",
        "\n",
        "#df['lemmatized_caption'] = df['lemmatized_caption'].astype(str)\n",
        "df['caption'] = df['caption'].astype(str)\n",
        "\n",
        "df['sentiment'] = df['caption'].apply(lambda x: get_sentiment(x))\n",
        "\n",
        "print(df['caption'])\n",
        "#print(df['caption'],df['sentiment'])\n",
        "\n",
        "df.to_csv('sentiment.csv', columns=['caption', 'sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MJvbQ-t5bEj"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(captions):\n",
        "  return analyzer.polarity_scores(captions)\n",
        "\n",
        "#df['lemmatized_caption'] = df['lemmatized_caption'].astype(str)\n",
        "df['caption'] = df['caption'].astype(str)\n",
        "\n",
        "df['sentiment'] = df['caption'].apply(lambda x: get_sentiment(x))\n",
        "\n",
        "print(df['caption'])\n",
        "#print(df['caption'],df['sentiment'])\n",
        "\n",
        "df.to_csv('sentiment.csv', columns=['caption', 'sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brPbOTOv55RZ"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "import pandas as pd\n",
        "nltk.download('vader_lexicon')\n",
        "user_input = 'harvardcomments.json'\n",
        "df = pd.read_json(user_input)\n",
        "\n",
        "\n",
        "\n",
        "#Creates Analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n",
        "\n",
        "def get_sentiment(text):\n",
        "  sentiments = analyzer.polarity_scores(text)\n",
        "  return sentiments['compound']\n",
        "\n",
        "df['text'] = df['text'].astype(str)\n",
        "\n",
        "df['sentiment'] = df['text'].apply(get_sentiment)\n",
        "export_input = 'harvard_comments_sentiment.csv'\n",
        "df.to_csv(export_input)\n",
        "print(df['sentiment'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H7P5EfUNHpo"
      },
      "source": [
        "# Sentiment Analysis using TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJw_9vQE7BKS"
      },
      "outputs": [],
      "source": [
        "!pip install textblob\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "\n",
        "df['lemmatized_caption'] = df['lemmatized_caption'].astype(str)\n",
        "\n",
        "blob = TextBlob(\"\".join(df['lemmatized_caption']))\n",
        "#blob.sentiment.polarity\n",
        "df['sentiment'] = df['lemmatized_caption'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "print(df['sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WXdEScKNKtn"
      },
      "outputs": [],
      "source": [
        "!pip install textblob\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "\n",
        "user_input = input(\"Enter the file name: \")\n",
        "df = pd.read_csv(user_input)\n",
        "\n",
        "df['comments'] = df['comments'].astype(str)\n",
        "\n",
        "blob = TextBlob(\"\".join(df['comments']))\n",
        "#blob.sentiment.polarity\n",
        "df['sentiment'] = df['comments'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "print(df['sentiment'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hhW3np_t6G3"
      },
      "source": [
        "#Sentiment Analysis Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0kEsjmtt4IP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "user_input = \"startup_comments_sentiment.csv\"\n",
        "df = pd.read_csv(user_input)\n",
        "\n",
        "comments = list(range(0,len(df['sentiment'])))\n",
        "#plt.bar(captions, df['sentiment'])\n",
        "plt.scatter(comments, df['sentiment'])\n",
        "plt.xlabel = 'Comment'\n",
        "plt.ylabel = 'Sentiment Score'\n",
        "plt.title = 'Sentiment Analysis of Captions'\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "thGMd2sAtuDv",
        "z0-8mhq48Gv5",
        "oZRKAQ_ykE11",
        "9alonUuQtLUZ",
        "WwrNPyX-1tUv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}